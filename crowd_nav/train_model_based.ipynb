{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9cabbf-37f2-4b6b-a40d-655e9a95053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import gym\n",
    "import git\n",
    "import sys, os, pickle\n",
    "sys.path.append('../')\n",
    "from crowd_sim.envs.utils.robot import Robot\n",
    "from crowd_nav.utils.trainer import Trainer\n",
    "from crowd_nav.utils.trainer_sim import Trainer_Sim\n",
    "from crowd_nav.utils.memory import ReplayMemory\n",
    "from crowd_nav.utils.explorer import Explorer\n",
    "from crowd_nav.policy.policy_factory import policy_factory\n",
    "from crowd_nav.policy.world_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03254014-8419-422b-8bf4-64c6b881de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('Parse configuration file')\n",
    "parser.add_argument('--env_config', type=str, default='configs_test/env.config')\n",
    "parser.add_argument('--policy', type=str, default='sarl')\n",
    "parser.add_argument('--policy_config', type=str, default='configs_test/policy.config')\n",
    "parser.add_argument('--train_config', type=str, default='configs_test/train.config')\n",
    "parser.add_argument('--output_dir', type=str, default='data/sarl5')\n",
    "parser.add_argument('--weights', type=str)\n",
    "parser.add_argument('--resume', default=False, action='store_true')\n",
    "parser.add_argument('--gpu', default=False, action='store_true')\n",
    "parser.add_argument('--debug', default=False, action='store_true')\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a4cfe3-fe94-41e1-bcd6-b4f18f6a977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Output directory already exists! Overwrite the folder? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 15:08:21, INFO: Current git head hash code: %s\n",
      "2022-10-19 15:08:21, INFO: Using device: cuda:0\n",
      "2022-10-19 15:08:21, INFO: Policy: SARL w/ global state\n",
      "2022-10-19 15:08:25, INFO: human number: 5\n",
      "2022-10-19 15:08:25, INFO: Not randomize human's radius and preferred speed\n",
      "2022-10-19 15:08:25, INFO: Training simulation: circle_crossing, test simulation: circle_crossing\n",
      "2022-10-19 15:08:25, INFO: Square width: 10.0, circle width: 4.0\n",
      "2022-10-19 15:08:25, INFO: human number: 5\n",
      "2022-10-19 15:08:25, INFO: Not randomize human's radius and preferred speed\n",
      "2022-10-19 15:08:25, INFO: Training simulation: circle_crossing, test simulation: circle_crossing\n",
      "2022-10-19 15:08:25, INFO: Square width: 10.0, circle width: 4.0\n"
     ]
    }
   ],
   "source": [
    "# configure paths\n",
    "make_new_dir = True\n",
    "if os.path.exists(args.output_dir):\n",
    "    key = input('Output directory already exists! Overwrite the folder? (y/n)')\n",
    "    if key == 'y' and not args.resume:\n",
    "        shutil.rmtree(args.output_dir)\n",
    "    else:\n",
    "        make_new_dir = False\n",
    "        args.env_config = os.path.join(args.output_dir, os.path.basename(args.env_config))\n",
    "        args.policy_config = os.path.join(args.output_dir, os.path.basename(args.policy_config))\n",
    "        args.train_config = os.path.join(args.output_dir, os.path.basename(args.train_config))\n",
    "if make_new_dir:\n",
    "    os.makedirs(args.output_dir)\n",
    "    shutil.copy(args.env_config, args.output_dir)\n",
    "    shutil.copy(args.policy_config, args.output_dir)\n",
    "    shutil.copy(args.train_config, args.output_dir)\n",
    "log_file = os.path.join(args.output_dir, 'output.log')\n",
    "il_weight_file = os.path.join(args.output_dir, 'il_model.pth')\n",
    "rl_weight_file = os.path.join(args.output_dir, 'rl_model.pth')\n",
    "\n",
    "# configure logging\n",
    "mode = 'a' if args.resume else 'w'\n",
    "file_handler = logging.FileHandler(log_file, mode=mode)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "level = logging.INFO if not args.debug else logging.DEBUG\n",
    "logging.basicConfig(level=level, handlers=[stdout_handler, file_handler],\n",
    "                    format='%(asctime)s, %(levelname)s: %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "logging.info('Current git head hash code: %s'.format(repo.head.object.hexsha))\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() and args.gpu else \"cpu\")\n",
    "device = torch.device(args.device )\n",
    "logging.info('Using device: %s', device)\n",
    "mem_path = os.path.join(args.output_dir, 'memory.data')\n",
    "rawob_path = os.path.join(args.output_dir, 'rawob.data')\n",
    "model_sim_checkpoint = os.path.join(args.output_dir, 'model_sim.pt')\n",
    "# configure policy\n",
    "policy = policy_factory[args.policy]()\n",
    "if not policy.trainable:\n",
    "    parser.error('Policy has to be trainable')\n",
    "if args.policy_config is None:\n",
    "    parser.error('Policy config has to be specified for a trainable network')\n",
    "policy_config = configparser.RawConfigParser()\n",
    "policy_config.read(args.policy_config)\n",
    "policy.configure(policy_config)\n",
    "policy.set_device(device)\n",
    "\n",
    "# configure environment\n",
    "env_config = configparser.RawConfigParser()\n",
    "env_config.read(args.env_config)\n",
    "env = gym.make('CrowdSim-v0')\n",
    "env.configure(env_config)\n",
    "robot = Robot(env_config, 'robot')\n",
    "env.set_robot(robot)\n",
    "\n",
    "\n",
    "\n",
    "# read training parameters\n",
    "if args.train_config is None:\n",
    "    parser.error('Train config has to be specified for a trainable network')\n",
    "train_config = configparser.RawConfigParser()\n",
    "train_config.read(args.train_config)\n",
    "rl_learning_rate = train_config.getfloat('train', 'rl_learning_rate')\n",
    "train_batches = train_config.getint('train', 'train_batches')\n",
    "train_episodes = train_config.getint('train', 'train_episodes')\n",
    "sample_episodes = train_config.getint('train', 'sample_episodes')\n",
    "target_update_interval = train_config.getint('train', 'target_update_interval')\n",
    "evaluation_interval = train_config.getint('train', 'evaluation_interval')\n",
    "capacity = train_config.getint('train', 'capacity')\n",
    "epsilon_start = train_config.getfloat('train', 'epsilon_start')\n",
    "epsilon_end = train_config.getfloat('train', 'epsilon_end')\n",
    "epsilon_decay = train_config.getfloat('train', 'epsilon_decay')\n",
    "checkpoint_interval = train_config.getint('train', 'checkpoint_interval')\n",
    "\n",
    "init_episodes = train_config.getint('train_sim', 'init_episodes')\n",
    "model_sim_lr = train_config.getfloat('train_sim', 'model_sim_lr')\n",
    "model_sim_epochs = train_config.getint('train_sim', 'model_sim_epochs')\n",
    "ms_batchsize = train_config.getint('train_sim', 'ms_batchsize')\n",
    "\n",
    "# configure trainer and explorer\n",
    "memory = ReplayMemory(capacity)\n",
    "model = policy.get_model()\n",
    "batch_size = train_config.getint('trainer', 'batch_size')\n",
    "trainer = Trainer(model, memory, device, batch_size)\n",
    "explorer = Explorer(env, robot, device, memory, policy.gamma, target_policy=policy)\n",
    "explorer.rawob = ReplayMemory(capacity)\n",
    "\n",
    "# config sim environment\n",
    "model_sim = autoencoder(env_config.getint('sim', 'human_num')); model_sim.to(device)\n",
    "env_sim = gym.make('ModelCrowdSim-v0')\n",
    "env_sim.configure(env_config)\n",
    "env_sim.set_robot(robot)\n",
    "env_sim.device = device\n",
    "env_sim.sim_world= model_sim\n",
    "# model based things\n",
    "trainer_sim = Trainer_Sim(model_sim, explorer.rawob, device, ms_batchsize)\n",
    "trainer_sim.path = model_sim_checkpoint\n",
    "explorer_sim = Explorer(env_sim, robot, device, memory, policy.gamma, target_policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2374495-5bbd-4a4e-9f31-a11529c24e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 15:08:28, INFO: TRAIN has success rate: 0.84, collision rate: 0.14, nav time: 12.54, total reward: 0.2077\n",
      "2022-10-19 15:08:28, INFO: Saving memory: data/sarl5/memory.data\n",
      "2022-10-19 15:08:28, INFO: Saving raw observation: data/sarl5/rawob.data\n",
      "2022-10-19 15:08:29, INFO: Current learning rate: 0.001000\n",
      "2022-10-19 15:08:30, INFO: Finish init model_sim. val_loss: 0.4028\n"
     ]
    }
   ],
   "source": [
    "# Sample data for model training\n",
    "il_episodes = train_config.getint('imitation_learning', 'il_episodes')\n",
    "il_policy = train_config.get('imitation_learning', 'il_policy')\n",
    "il_epochs = train_config.getint('imitation_learning', 'il_epochs')\n",
    "il_learning_rate = train_config.getfloat('imitation_learning', 'il_learning_rate')\n",
    "if robot.visible:\n",
    "    safety_space = 0\n",
    "else:\n",
    "    safety_space = train_config.getfloat('imitation_learning', 'safety_space')\n",
    "il_policy = policy_factory[il_policy]()\n",
    "il_policy.multiagent_training = policy.multiagent_training\n",
    "il_policy.safety_space = safety_space\n",
    "robot.set_policy(il_policy)\n",
    "\n",
    "# sample data from real env\n",
    "explorer.run_k_episodes(init_episodes, 'train', update_memory=False, imitation_learning=True,update_raw_ob=True)\n",
    "\n",
    "# Saving memory\n",
    "logging.info(\"Saving memory: %s\",mem_path)\n",
    "with open(mem_path, 'wb') as f:\n",
    "    pickle.dump(memory,f)\n",
    "logging.info(\"Saving raw observation: %s\",rawob_path)\n",
    "with open(rawob_path, 'wb') as f:\n",
    "    pickle.dump(explorer.rawob,f)\n",
    "    \n",
    "# training sim model\n",
    "trainer_sim.set_learning_rate(model_sim_lr)\n",
    "ms_valid_loss = trainer_sim.optimize_epoch(model_sim_epochs)\n",
    "logging.info('Finish init model_sim. val_loss: {:.4f}'.format(-ms_valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9f55cb-0a96-4848-95fa-aece75f70dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 15:09:16, INFO: TRAIN has success rate: 0.64, collision rate: 0.36, nav time: 8.47, total reward: 0.1915\n",
      "2022-10-19 15:09:16, INFO: Current learning rate: 0.010000\n",
      "2022-10-19 15:10:01, INFO: Finish imitation learning. Weights saved.\n",
      "2022-10-19 15:10:01, INFO: Experience set size: 12623/100000\n"
     ]
    }
   ],
   "source": [
    "# imitation learning\n",
    "explorer_sim.run_k_episodes(il_episodes, 'train', update_memory=True, imitation_learning=True)\n",
    "trainer.set_learning_rate(il_learning_rate)\n",
    "trainer.optimize_epoch(il_epochs)\n",
    "torch.save(model.state_dict(), il_weight_file)\n",
    "logging.info('Finish imitation learning. Weights saved.')\n",
    "logging.info('Experience set size: %d/%d', len(memory), memory.capacity)\n",
    "explorer_sim.update_target_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3d1410-f526-4798-ba9e-afe06daa8b64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 15:10:01, INFO: Agent is invisible and has holonomic kinematic constraint\n",
      "2022-10-19 15:10:01, INFO: Current learning rate: 0.001000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Value network is not well trained. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0f8931356d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# training sim model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mexplorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_k_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_raw_ob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtrainer_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/phd/CrowdNav/crowd_nav/utils/explorer.py\u001b[0m in \u001b[0;36mrun_k_episodes\u001b[0;34m(self, k, phase, update_memory, imitation_learning, episode, print_failure, update_raw_ob)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mcurrent_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mcurrent_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtmpo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtmpo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/phd/CrowdNav/crowd_sim/envs/utils/robot.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, ob)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Policy attribute has to be set!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJointState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/phd/CrowdNav/crowd_nav/policy/multi_human_rl.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0mmax_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Value network is not well trained. '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Value network is not well trained. "
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "policy.set_env(env_sim)\n",
    "robot.set_policy(policy)\n",
    "robot.print_info()\n",
    "trainer.set_learning_rate(rl_learning_rate)\n",
    "episode = 0 \n",
    "\n",
    "# while episode < train_episodes:\n",
    "while episode < 100:\n",
    "    if args.resume:\n",
    "        epsilon = epsilon_end\n",
    "    else:\n",
    "        if episode < epsilon_decay:\n",
    "            epsilon = epsilon_start + (epsilon_end - epsilon_start) / epsilon_decay * episode\n",
    "        else:\n",
    "            epsilon = epsilon_end\n",
    "    robot.policy.set_epsilon(epsilon)\n",
    "\n",
    "    # # evaluate the model\n",
    "    # if episode % evaluation_interval == 0:\n",
    "    #     explorer_sim.run_k_episodes(env.case_size['val'], 'val', episode=episode)\n",
    "        \n",
    "    # training sim model\n",
    "    explorer.run_k_episodes(sample_episodes, 'train', update_memory=False, update_raw_ob=True)\n",
    "    trainer_sim.optimize_epoch(il_epochs)\n",
    "\n",
    "    # sample k episodes into memory and optimize over the generated memory\n",
    "    explorer_sim.run_k_episodes(sample_episodes, 'train', update_memory=True, episode=episode)\n",
    "    trainer.optimize_batch(train_batches)\n",
    "    episode += 1\n",
    "\n",
    "    if episode % target_update_interval == 0:\n",
    "        explorer_sim.update_target_model(model)\n",
    "\n",
    "    if episode != 0 and episode % checkpoint_interval == 0:\n",
    "        torch.save(model.state_dict(), rl_weight_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c70be-da61-40de-89fc-6cb43c6de1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final test\n",
    "explorer.run_k_episodes(env.case_size['test'], 'test', episode=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7c1d4-8e41-4d79-8caf-9cf279d82ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maml",
   "language": "python",
   "name": "maml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
