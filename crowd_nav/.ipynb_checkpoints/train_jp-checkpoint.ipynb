{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421b70e6-710f-46ff-a048-12f44c0fda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import configparser\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import gym\n",
    "import git\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from crowd_sim.envs.utils.robot import Robot\n",
    "from crowd_nav.utils.trainer import Trainer\n",
    "from crowd_nav.utils.memory import ReplayMemory\n",
    "from crowd_nav.utils.explorer import Explorer\n",
    "from crowd_nav.policy.policy_factory import policy_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6698d8d7-eb84-46a6-813a-f866bcc65e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('Parse configuration file')\n",
    "parser.add_argument('--env_config', type=str, default='configs/env5.config')\n",
    "parser.add_argument('--policy', type=str, default='sarl')\n",
    "parser.add_argument('--policy_config', type=str, default='configs/policy.config')\n",
    "parser.add_argument('--train_config', type=str, default='configs/train.config')\n",
    "parser.add_argument('--output_dir', type=str, default='data/sarl5')\n",
    "parser.add_argument('--weights', type=str)\n",
    "parser.add_argument('--resume', default=False, action='store_true')\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "parser.add_argument('--debug', default=False, action='store_true')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0cfcfab-cb3c-4560-97ce-f45acfcd411b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Output directory already exists! Overwrite the folder? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-10 15:19:44, INFO: Current git head hash code: %s\n",
      "2022-10-10 15:19:44, INFO: Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# configure paths\n",
    "make_new_dir = True\n",
    "if os.path.exists(args.output_dir):\n",
    "    key = input('Output directory already exists! Overwrite the folder? (y/n)')\n",
    "    if key == 'y' and not args.resume:\n",
    "        shutil.rmtree(args.output_dir)\n",
    "    else:\n",
    "        make_new_dir = False\n",
    "        args.env_config = os.path.join(args.output_dir, os.path.basename(args.env_config))\n",
    "        args.policy_config = os.path.join(args.output_dir, os.path.basename(args.policy_config))\n",
    "        args.train_config = os.path.join(args.output_dir, os.path.basename(args.train_config))\n",
    "if make_new_dir:\n",
    "    os.makedirs(args.output_dir)\n",
    "    shutil.copy(args.env_config, args.output_dir)\n",
    "    shutil.copy(args.policy_config, args.output_dir)\n",
    "    shutil.copy(args.train_config, args.output_dir)\n",
    "log_file = os.path.join(args.output_dir, 'output.log')\n",
    "il_weight_file = os.path.join(args.output_dir, 'il_model.pth')\n",
    "rl_weight_file = os.path.join(args.output_dir, 'rl_model.pth')\n",
    "mem_path = os.path.join(args.output_dir, 'memory.data')\n",
    "\n",
    "# configure logging\n",
    "mode = 'a' if args.resume else 'w'\n",
    "file_handler = logging.FileHandler(log_file, mode=mode)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "level = logging.INFO if not args.debug else logging.DEBUG\n",
    "logging.basicConfig(level=level, handlers=[stdout_handler, file_handler],\n",
    "                    format='%(asctime)s, %(levelname)s: %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "logging.info('Current git head hash code: %s'.format(repo.head.object.hexsha))\n",
    "device = torch.device(args.device )\n",
    "logging.info('Using device: %s', device)\n",
    "\n",
    "# configure policy\n",
    "policy = policy_factory[args.policy]()\n",
    "if not policy.trainable:\n",
    "    parser.error('Policy has to be trainable')\n",
    "if args.policy_config is None:\n",
    "    parser.error('Policy config has to be specified for a trainable network')\n",
    "policy_config = configparser.RawConfigParser()\n",
    "policy_config.read(args.policy_config)\n",
    "policy.configure(policy_config)\n",
    "policy.set_device(device)\n",
    "\n",
    "# configure environment\n",
    "env_config = configparser.RawConfigParser()\n",
    "env_config.read(args.env_config)\n",
    "env = gym.make('CrowdSim-v0')\n",
    "env.configure(env_config)\n",
    "robot = Robot(env_config, 'robot')\n",
    "env.set_robot(robot)\n",
    "\n",
    "# read training parameters\n",
    "if args.train_config is None:\n",
    "    parser.error('Train config has to be specified for a trainable network')\n",
    "train_config = configparser.RawConfigParser()\n",
    "train_config.read(args.train_config)\n",
    "rl_learning_rate = train_config.getfloat('train', 'rl_learning_rate')\n",
    "train_batches = train_config.getint('train', 'train_batches')\n",
    "train_episodes = train_config.getint('train', 'train_episodes')\n",
    "sample_episodes = train_config.getint('train', 'sample_episodes')\n",
    "target_update_interval = train_config.getint('train', 'target_update_interval')\n",
    "evaluation_interval = train_config.getint('train', 'evaluation_interval')\n",
    "capacity = train_config.getint('train', 'capacity')\n",
    "epsilon_start = train_config.getfloat('train', 'epsilon_start')\n",
    "epsilon_end = train_config.getfloat('train', 'epsilon_end')\n",
    "epsilon_decay = train_config.getfloat('train', 'epsilon_decay')\n",
    "checkpoint_interval = train_config.getint('train', 'checkpoint_interval')\n",
    "\n",
    "# configure trainer and explorer\n",
    "memory = ReplayMemory(capacity)\n",
    "model = policy.get_model()\n",
    "batch_size = train_config.getint('trainer', 'batch_size')\n",
    "trainer = Trainer(model, memory, device, batch_size)\n",
    "explorer = Explorer(env, robot, device, memory, policy.gamma, target_policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645c94e7-0d44-4ac6-bc0d-103cf6f0d374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reset() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12048/660784899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mil_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafety_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafety_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mexplorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_k_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimitation_learning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mil_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mil_weight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/phd/CrowdNav/crowd_nav/../crowd_nav/utils/explorer.py\u001b[0m in \u001b[0;36mrun_k_episodes\u001b[0;34m(self, k, phase, update_memory, imitation_learning, episode, print_failure, test_case, e_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtimeout_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reset() takes 1 positional argument but 3 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/minhrobot/.conda/envs/meta/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3810, in atexit_operations\n",
      "    self.reset(new_session=False)\n",
      "  File \"/home/minhrobot/.conda/envs/meta/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1421, in reset\n",
      "    self.history_manager.reset(new_session)\n",
      "  File \"/home/minhrobot/.conda/envs/meta/lib/python3.8/site-packages/IPython/core/history.py\", line 592, in reset\n",
      "    self.dir_hist[:] = [os.getcwd()]\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# imitation learning\n",
    "if args.resume:\n",
    "    if not os.path.exists(rl_weight_file):\n",
    "        logging.error('RL weights does not exist')\n",
    "    model.load_state_dict(torch.load(rl_weight_file))\n",
    "    rl_weight_file = os.path.join(args.output_dir, 'resumed_rl_model.pth')\n",
    "    logging.info('Load reinforcement learning trained weights. Resume training')\n",
    "elif os.path.exists(il_weight_file):\n",
    "    model.load_state_dict(torch.load(il_weight_file))\n",
    "    logging.info('Load imitation learning trained weights.')\n",
    "else:\n",
    "    il_episodes = train_config.getint('imitation_learning', 'il_episodes')\n",
    "    il_policy = train_config.get('imitation_learning', 'il_policy')\n",
    "    il_epochs = train_config.getint('imitation_learning', 'il_epochs')\n",
    "    il_learning_rate = train_config.getfloat('imitation_learning', 'il_learning_rate')\n",
    "    trainer.set_learning_rate(il_learning_rate)\n",
    "    if robot.visible:\n",
    "        safety_space = 0\n",
    "    else:\n",
    "        safety_space = train_config.getfloat('imitation_learning', 'safety_space')\n",
    "    il_policy = policy_factory[il_policy]()\n",
    "    il_policy.multiagent_training = policy.multiagent_training\n",
    "    il_policy.safety_space = safety_space\n",
    "    robot.set_policy(il_policy)\n",
    "    explorer.run_k_episodes(il_episodes, 'train', update_memory=True, imitation_learning=True)\n",
    "    trainer.optimize_epoch(il_epochs)\n",
    "    torch.save(model.state_dict(), il_weight_file)\n",
    "    logging.info('Finish imitation learning. Weights saved.')\n",
    "    logging.info('Experience set size: %d/%d', len(memory), memory.capacity)\n",
    "explorer.update_target_model(model)\n",
    "\n",
    "# pickle memory\n",
    "logging.info(\"Saving memory: %s\",mem_path)\n",
    "with open(mem_path, 'wb') as f:\n",
    "    pickle.dump(memory,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b17ea2a-720f-43a5-8aae-2c24f204772d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-20 17:53:59, INFO: Agent is invisible and has holonomic kinematic constraint\n",
      "2022-02-20 17:53:59, INFO: Current learning rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "# reinforcement learning\n",
    "policy.set_env(env)\n",
    "robot.set_policy(policy)\n",
    "robot.print_info()\n",
    "trainer.set_learning_rate(rl_learning_rate)\n",
    "# fill the memory pool with some RL experience\n",
    "if args.resume:\n",
    "    robot.policy.set_epsilon(epsilon_end)\n",
    "    explorer.run_k_episodes(10, 'train', update_memory=True, episode=0)\n",
    "    logging.info('Experience set size: %d/%d', len(memory), memory.capacity)\n",
    "episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48eb4dae-f72a-4b9d-8664-b97ec4535571",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-20 17:54:17, INFO: TRAIN in episode 0 has success rate: 1.00, collision rate: 0.00, nav time: 18.25, total reward: 0.1501\n",
      "2022-02-20 17:54:42, INFO: TRAIN in episode 1 has success rate: 0.00, collision rate: 0.00, nav time: 25.00, total reward: 0.0000\n",
      "2022-02-20 17:55:02, INFO: TRAIN in episode 2 has success rate: 1.00, collision rate: 0.00, nav time: 20.25, total reward: 0.1216\n",
      "2022-02-20 17:55:24, INFO: TRAIN in episode 3 has success rate: 1.00, collision rate: 0.00, nav time: 21.00, total reward: 0.1123\n",
      "2022-02-20 17:55:42, INFO: TRAIN in episode 4 has success rate: 1.00, collision rate: 0.00, nav time: 21.00, total reward: 0.1123\n",
      "2022-02-20 17:56:02, INFO: TRAIN in episode 5 has success rate: 1.00, collision rate: 0.00, nav time: 20.25, total reward: 0.1216\n",
      "2022-02-20 17:56:10, INFO: TRAIN in episode 6 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1260\n",
      "2022-02-20 17:56:30, INFO: TRAIN in episode 7 has success rate: 1.00, collision rate: 0.00, nav time: 23.50, total reward: 0.0863\n",
      "2022-02-20 17:56:32, INFO: TRAIN in episode 8 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.2830\n",
      "2022-02-20 17:56:52, INFO: TRAIN in episode 9 has success rate: 1.00, collision rate: 0.00, nav time: 21.50, total reward: 0.1066\n",
      "2022-02-20 17:56:53, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 17:56:58, INFO: TRAIN in episode 10 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1473\n",
      "2022-02-20 17:57:07, INFO: TRAIN in episode 11 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1259\n",
      "2022-02-20 17:57:26, INFO: TRAIN in episode 12 has success rate: 1.00, collision rate: 0.00, nav time: 21.00, total reward: 0.1123\n",
      "2022-02-20 17:57:43, INFO: TRAIN in episode 13 has success rate: 1.00, collision rate: 0.00, nav time: 19.25, total reward: 0.1225\n",
      "2022-02-20 17:57:50, INFO: TRAIN in episode 14 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1658\n",
      "2022-02-20 17:58:07, INFO: TRAIN in episode 15 has success rate: 1.00, collision rate: 0.00, nav time: 20.00, total reward: 0.0841\n",
      "2022-02-20 17:58:29, INFO: TRAIN in episode 16 has success rate: 1.00, collision rate: 0.00, nav time: 18.25, total reward: 0.1501\n",
      "2022-02-20 17:58:46, INFO: TRAIN in episode 17 has success rate: 1.00, collision rate: 0.00, nav time: 16.75, total reward: 0.1758\n",
      "2022-02-20 17:59:04, INFO: TRAIN in episode 18 has success rate: 1.00, collision rate: 0.00, nav time: 16.50, total reward: 0.1805\n",
      "2022-02-20 17:59:10, INFO: TRAIN in episode 19 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1369\n",
      "2022-02-20 17:59:12, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 17:59:17, INFO: TRAIN in episode 20 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1242\n",
      "2022-02-20 17:59:35, INFO: TRAIN in episode 21 has success rate: 1.00, collision rate: 0.00, nav time: 20.50, total reward: 0.1184\n",
      "2022-02-20 17:59:51, INFO: TRAIN in episode 22 has success rate: 1.00, collision rate: 0.00, nav time: 17.25, total reward: 0.1668\n",
      "2022-02-20 18:00:07, INFO: TRAIN in episode 23 has success rate: 1.00, collision rate: 0.00, nav time: 21.50, total reward: 0.1057\n",
      "2022-02-20 18:00:13, INFO: TRAIN in episode 24 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1492\n",
      "2022-02-20 18:00:33, INFO: TRAIN in episode 25 has success rate: 1.00, collision rate: 0.00, nav time: 15.25, total reward: 0.2059\n",
      "2022-02-20 18:00:53, INFO: TRAIN in episode 26 has success rate: 1.00, collision rate: 0.00, nav time: 18.00, total reward: 0.1541\n",
      "2022-02-20 18:01:11, INFO: TRAIN in episode 27 has success rate: 1.00, collision rate: 0.00, nav time: 20.75, total reward: 0.1153\n",
      "2022-02-20 18:01:33, INFO: TRAIN in episode 28 has success rate: 1.00, collision rate: 0.00, nav time: 17.25, total reward: 0.1668\n",
      "2022-02-20 18:01:53, INFO: TRAIN in episode 29 has success rate: 1.00, collision rate: 0.00, nav time: 24.00, total reward: 0.0819\n",
      "2022-02-20 18:01:55, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:02:01, INFO: TRAIN in episode 30 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1463\n",
      "2022-02-20 18:02:08, INFO: TRAIN in episode 31 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1579\n",
      "2022-02-20 18:02:28, INFO: TRAIN in episode 32 has success rate: 1.00, collision rate: 0.00, nav time: 20.25, total reward: 0.1216\n",
      "2022-02-20 18:02:52, INFO: TRAIN in episode 33 has success rate: 1.00, collision rate: 0.00, nav time: 19.50, total reward: 0.1316\n",
      "2022-02-20 18:03:09, INFO: TRAIN in episode 34 has success rate: 0.00, collision rate: 0.00, nav time: 25.00, total reward: 0.0000\n",
      "2022-02-20 18:03:30, INFO: TRAIN in episode 35 has success rate: 0.00, collision rate: 0.00, nav time: 25.00, total reward: 0.0000\n",
      "2022-02-20 18:03:47, INFO: TRAIN in episode 36 has success rate: 1.00, collision rate: 0.00, nav time: 16.50, total reward: 0.1805\n",
      "2022-02-20 18:04:06, INFO: TRAIN in episode 37 has success rate: 1.00, collision rate: 0.00, nav time: 16.75, total reward: 0.1758\n",
      "2022-02-20 18:04:25, INFO: TRAIN in episode 38 has success rate: 1.00, collision rate: 0.00, nav time: 18.25, total reward: 0.1501\n",
      "2022-02-20 18:04:34, INFO: TRAIN in episode 39 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1596\n",
      "2022-02-20 18:04:36, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:04:56, INFO: TRAIN in episode 40 has success rate: 1.00, collision rate: 0.00, nav time: 20.75, total reward: 0.1153\n",
      "2022-02-20 18:05:15, INFO: TRAIN in episode 41 has success rate: 1.00, collision rate: 0.00, nav time: 20.50, total reward: 0.1011\n",
      "2022-02-20 18:05:22, INFO: TRAIN in episode 42 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1505\n",
      "2022-02-20 18:05:40, INFO: TRAIN in episode 43 has success rate: 1.00, collision rate: 0.00, nav time: 20.50, total reward: 0.1184\n",
      "2022-02-20 18:05:49, INFO: TRAIN in episode 44 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1468\n",
      "2022-02-20 18:06:09, INFO: TRAIN in episode 45 has success rate: 1.00, collision rate: 0.00, nav time: 18.25, total reward: 0.1501\n",
      "2022-02-20 18:06:18, INFO: TRAIN in episode 46 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1520\n",
      "2022-02-20 18:06:40, INFO: TRAIN in episode 47 has success rate: 1.00, collision rate: 0.00, nav time: 23.00, total reward: 0.0804\n",
      "2022-02-20 18:07:02, INFO: TRAIN in episode 48 has success rate: 1.00, collision rate: 0.00, nav time: 20.50, total reward: 0.1184\n",
      "2022-02-20 18:07:12, INFO: TRAIN in episode 49 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1429\n",
      "2022-02-20 18:07:15, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:07:20, INFO: TRAIN in episode 50 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1433\n",
      "2022-02-20 18:07:40, INFO: TRAIN in episode 51 has success rate: 1.00, collision rate: 0.00, nav time: 17.50, total reward: 0.1624\n",
      "2022-02-20 18:07:59, INFO: TRAIN in episode 52 has success rate: 1.00, collision rate: 0.00, nav time: 19.25, total reward: 0.1351\n",
      "2022-02-20 18:08:21, INFO: TRAIN in episode 53 has success rate: 1.00, collision rate: 0.00, nav time: 22.25, total reward: 0.0985\n",
      "2022-02-20 18:08:39, INFO: TRAIN in episode 54 has success rate: 1.00, collision rate: 0.00, nav time: 19.00, total reward: 0.1387\n",
      "2022-02-20 18:09:01, INFO: TRAIN in episode 55 has success rate: 1.00, collision rate: 0.00, nav time: 19.75, total reward: 0.1282\n",
      "2022-02-20 18:09:20, INFO: TRAIN in episode 56 has success rate: 0.00, collision rate: 0.00, nav time: 25.00, total reward: 0.0000\n",
      "2022-02-20 18:09:46, INFO: TRAIN in episode 57 has success rate: 1.00, collision rate: 0.00, nav time: 24.00, total reward: 0.0819\n",
      "2022-02-20 18:10:03, INFO: TRAIN in episode 58 has success rate: 1.00, collision rate: 0.00, nav time: 17.75, total reward: 0.1582\n",
      "2022-02-20 18:10:22, INFO: TRAIN in episode 59 has success rate: 1.00, collision rate: 0.00, nav time: 19.00, total reward: 0.1377\n",
      "2022-02-20 18:10:25, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:10:42, INFO: TRAIN in episode 60 has success rate: 1.00, collision rate: 0.00, nav time: 17.50, total reward: 0.1624\n",
      "2022-02-20 18:10:50, INFO: TRAIN in episode 61 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1284\n",
      "2022-02-20 18:11:07, INFO: TRAIN in episode 62 has success rate: 1.00, collision rate: 0.00, nav time: 16.25, total reward: 0.1381\n",
      "2022-02-20 18:11:26, INFO: TRAIN in episode 63 has success rate: 1.00, collision rate: 0.00, nav time: 21.25, total reward: 0.1094\n",
      "2022-02-20 18:11:33, INFO: TRAIN in episode 64 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1343\n",
      "2022-02-20 18:11:54, INFO: TRAIN in episode 65 has success rate: 1.00, collision rate: 0.00, nav time: 21.00, total reward: 0.1123\n",
      "2022-02-20 18:12:12, INFO: TRAIN in episode 66 has success rate: 1.00, collision rate: 0.00, nav time: 20.75, total reward: 0.1153\n",
      "2022-02-20 18:12:19, INFO: TRAIN in episode 67 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1352\n",
      "2022-02-20 18:12:29, INFO: TRAIN in episode 68 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1185\n",
      "2022-02-20 18:12:36, INFO: TRAIN in episode 69 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1502\n",
      "2022-02-20 18:12:38, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:12:48, INFO: TRAIN in episode 70 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.0860\n",
      "2022-02-20 18:13:10, INFO: TRAIN in episode 71 has success rate: 1.00, collision rate: 0.00, nav time: 22.25, total reward: 0.0985\n",
      "2022-02-20 18:13:17, INFO: TRAIN in episode 72 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1364\n",
      "2022-02-20 18:13:24, INFO: TRAIN in episode 73 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1410\n",
      "2022-02-20 18:13:44, INFO: TRAIN in episode 74 has success rate: 1.00, collision rate: 0.00, nav time: 20.25, total reward: 0.1216\n",
      "2022-02-20 18:14:04, INFO: TRAIN in episode 75 has success rate: 1.00, collision rate: 0.00, nav time: 21.00, total reward: 0.1123\n",
      "2022-02-20 18:14:25, INFO: TRAIN in episode 76 has success rate: 1.00, collision rate: 0.00, nav time: 19.50, total reward: 0.1316\n",
      "2022-02-20 18:14:43, INFO: TRAIN in episode 77 has success rate: 1.00, collision rate: 0.00, nav time: 18.00, total reward: 0.1226\n",
      "2022-02-20 18:15:02, INFO: TRAIN in episode 78 has success rate: 1.00, collision rate: 0.00, nav time: 15.25, total reward: 0.2033\n",
      "2022-02-20 18:15:22, INFO: TRAIN in episode 79 has success rate: 1.00, collision rate: 0.00, nav time: 20.75, total reward: 0.1153\n",
      "2022-02-20 18:15:24, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:15:29, INFO: TRAIN in episode 80 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1653\n",
      "2022-02-20 18:15:38, INFO: TRAIN in episode 81 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1543\n",
      "2022-02-20 18:16:01, INFO: TRAIN in episode 82 has success rate: 1.00, collision rate: 0.00, nav time: 19.50, total reward: 0.1316\n",
      "2022-02-20 18:16:23, INFO: TRAIN in episode 83 has success rate: 1.00, collision rate: 0.00, nav time: 21.00, total reward: 0.1123\n",
      "2022-02-20 18:16:45, INFO: TRAIN in episode 84 has success rate: 0.00, collision rate: 0.00, nav time: 25.00, total reward: 0.0000\n",
      "2022-02-20 18:17:03, INFO: TRAIN in episode 85 has success rate: 1.00, collision rate: 0.00, nav time: 19.00, total reward: 0.1005\n",
      "2022-02-20 18:17:20, INFO: TRAIN in episode 86 has success rate: 1.00, collision rate: 0.00, nav time: 19.25, total reward: 0.1351\n",
      "2022-02-20 18:17:43, INFO: TRAIN in episode 87 has success rate: 1.00, collision rate: 0.00, nav time: 19.50, total reward: 0.0794\n",
      "2022-02-20 18:18:01, INFO: TRAIN in episode 88 has success rate: 1.00, collision rate: 0.00, nav time: 19.50, total reward: 0.1240\n",
      "2022-02-20 18:18:11, INFO: TRAIN in episode 89 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1506\n",
      "2022-02-20 18:18:16, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:18:23, INFO: TRAIN in episode 90 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1105\n",
      "2022-02-20 18:18:46, INFO: TRAIN in episode 91 has success rate: 1.00, collision rate: 0.00, nav time: 22.50, total reward: 0.0959\n",
      "2022-02-20 18:19:09, INFO: TRAIN in episode 92 has success rate: 1.00, collision rate: 0.00, nav time: 21.50, total reward: 0.1066\n",
      "2022-02-20 18:19:21, INFO: TRAIN in episode 93 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1181\n",
      "2022-02-20 18:19:48, INFO: TRAIN in episode 94 has success rate: 1.00, collision rate: 0.00, nav time: 19.25, total reward: 0.0836\n",
      "2022-02-20 18:19:59, INFO: TRAIN in episode 95 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1329\n",
      "2022-02-20 18:20:23, INFO: TRAIN in episode 96 has success rate: 0.00, collision rate: 0.00, nav time: 25.00, total reward: -0.0001\n",
      "2022-02-20 18:20:26, INFO: TRAIN in episode 97 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.2786\n",
      "2022-02-20 18:20:47, INFO: TRAIN in episode 98 has success rate: 1.00, collision rate: 0.00, nav time: 19.50, total reward: 0.1316\n",
      "2022-02-20 18:21:07, INFO: TRAIN in episode 99 has success rate: 1.00, collision rate: 0.00, nav time: 15.00, total reward: 0.2114\n",
      "2022-02-20 18:21:10, INFO: Saving model to: data/test_train/rl_model.pth\n",
      "2022-02-20 18:21:32, INFO: TRAIN in episode 100 has success rate: 1.00, collision rate: 0.00, nav time: 24.00, total reward: 0.0819\n",
      "2022-02-20 18:21:51, INFO: TRAIN in episode 101 has success rate: 1.00, collision rate: 0.00, nav time: 21.50, total reward: 0.1066\n",
      "2022-02-20 18:21:59, INFO: TRAIN in episode 102 has success rate: 0.00, collision rate: 1.00, nav time: 25.00, total reward: -0.1767\n",
      "2022-02-20 18:22:21, INFO: TRAIN in episode 103 has success rate: 1.00, collision rate: 0.00, nav time: 22.00, total reward: 0.1011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4d8da1c7144d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# sample k episodes into memory and optimize over the generated memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mexplorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_k_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mepisode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/CrowdNav/crowd_nav/utils/trainer.py\u001b[0m in \u001b[0;36moptimize_batch\u001b[0;34m(self, num_batches)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/maml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/maml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/maml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/maml/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/maml/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while episode < train_episodes:\n",
    "    if args.resume:\n",
    "        epsilon = epsilon_end\n",
    "    else:\n",
    "        if episode < epsilon_decay:\n",
    "            epsilon = epsilon_start + (epsilon_end - epsilon_start) / epsilon_decay * episode\n",
    "        else:\n",
    "            epsilon = epsilon_end\n",
    "    robot.policy.set_epsilon(epsilon)\n",
    "\n",
    "    # evaluate the model\n",
    "    # if episode % evaluation_interval == 0:\n",
    "    #     explorer.run_k_episodes(env.case_size['val'], 'val', episode=episode)\n",
    "\n",
    "    # sample k episodes into memory and optimize over the generated memory\n",
    "    explorer.run_k_episodes(sample_episodes, 'train', update_memory=True, episode=episode)\n",
    "    trainer.optimize_batch(train_batches)\n",
    "    episode += 1\n",
    "\n",
    "    if episode % target_update_interval == 0:\n",
    "        explorer.update_target_model(model)\n",
    "\n",
    "    if episode != 0 and episode % checkpoint_interval == 0:\n",
    "        torch.save(model.state_dict(), rl_weight_file)\n",
    "        logging.info(\"Saving model to: %s\",rl_weight_file)\n",
    "\n",
    "# final test\n",
    "explorer.run_k_episodes(env.case_size['test'], 'test', episode=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852e1cb-e94f-40f6-9741-33aa67b50bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b8c78-ad3f-452c-ab5a-6073741f5048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta",
   "language": "python",
   "name": "meta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
